<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 実装演習レポート[応用数学]
## はじめに
    本レポートはrabbit challenge受講者「Manabu Kobatake」がまとめた
    応用数学に係る実装演習レポートです。
    各章の最後にまとめ（要約）を記載しておりますのでそちらをご覧ください。
    【無断転載禁止】
## 第1章：線形代数
### 1.行列
    ・スカラー：単一の数値
|   1   |
|-------|
|   x   |

    ・ベクトル：複数のスカラーの連なり
|  1 |  2 |  3 |  4 | ～ |  n |
|----|----|----|----|----|----|
| x1 | x2 | x3 | x4 | ～ | xn |
    ・行列：複数のベクトルの連なり
|  　 |  1 |  2 |  3 |  4 | ～ |  n |
|----|----|----|----|----|----|----|
|  1 | x11| x12| x13| x14| ～ | x1n|
|  2 | x21| x22| x23| x24| ～ | x2n|
|  3 | x31| x32| x33| x34| ～ | x3n|
| ～ | ～ | ～ | ～ | ～ | ～ | ～ |
|  m | xm1| xm2| xm3| xm4| ～ | xmn|
    ～行列とは連立方程式の解法の研究から発展して生まれた概念である。
    以下の連立1次方程式を基に解説する。
$$ {x_1}+2{x_2}=3　$$
$$ 2{x_1}+5{x_2}=8 $$
    上記の連立1次方程式を以下のベクトル式で表すと、係数a、未知数x、定数bで構成されることが分かる
$$ a・\vec{x}=\vec{b} $$
    この内、係数を表にまとめたものが行列に相当する。
    連立方程式をシンプルに表すための概念が行列と言える。
$$ 
\begin{pmatrix}
1 & 2 \\
2 & 5
\end{pmatrix} 
\begin{pmatrix}
{x_1} \\
{x_2}
\end{pmatrix} 
=
\begin{pmatrix}
3 \\
8
\end{pmatrix} 
$$
    上記式を解くことにより未知のベクトルxを求めることが可能となる。
    あるベクトルから違うベクトルに変換する際、乗算を行う。

### 行列とベクトルの積
    以下の行列×ベクトルの式の場合、
    ・第１成分：行列の1行目(の横ベクトル)×縦ベクトル
    ・第２成分：行列の2行目(の横ベクトル)×縦ベクトル
    を縦に列挙したベクトルになる。
$$ 
\begin{pmatrix}
6 & 4 \\
3 & 5
\end{pmatrix} 
\begin{pmatrix}
1 \\
2
\end{pmatrix} 
=
\begin{pmatrix}
6×1+4×2 \\
3×1+5×2
\end{pmatrix} 
=
\begin{pmatrix}
14 \\
13
\end{pmatrix} 
$$
    ここでの行列は元の古い第m成分から新しい第m成分を生み出すための係数に相当する。
    変換後の要素は元となる要素全てから影響を受ける。
    ニューラルネットワークの図を左右逆転させたものに等しい。
### 行列同士の積
    行列とはベクトルで構成されたベクトルである。
    ・行ベクトルが縦に並んでいる。
    ・列ベクトルが横に並んでいる。
$$
\vec{a1}
=
\begin{pmatrix}
6 & 4
\end{pmatrix}
,
\vec{a2}
=
\begin{pmatrix}
3 & 5
\end{pmatrix}
$$
$$
A
=
\begin{pmatrix}
\vec{a1} \\
\vec{a2}
\end{pmatrix} 
=
\begin{pmatrix}
6 & 4 \\
3 & 5
\end{pmatrix}
$$
    行列と列ベクトルの積により新たな成分が生まれる。
$$
\begin{pmatrix}
6 & 4 \\
3 & 5
\end{pmatrix}
\begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}
=
\begin{pmatrix}
6×1+4×3 & 6×2+4×4 \\
3×1+5×3 & 3×2+5×4
\end{pmatrix}
$$
$$
=
\begin{pmatrix}
18 & 28 \\
18 & 26
\end{pmatrix}
$$

### 連立方程式の解き方
    加減法により以下の連立方程式を解いてみる。
$$ {x_1}+2{x_2}=3$$
$$ 2{x_1}+5{x_2}=8 $$
    加減法では、係数の差異を平らかする。
    アプローチとしては以下の3種類で、この組み合わせにより未知数を求める。
    1.i行目をc倍する
    2.s行目にt行目のc倍を加算する
    3.i行目とj行目を入れ替える。

1.2行目に1行目の-2倍を加算
$$
{x_1}+2{x_2}=3
$$
$$
2{x_1}+5{x_2}=8   ←　-2×{x_1}-2×2{x_2}=-2×3
$$
$$
→{x_2}=2
$$

2.1行目に2行目の-2倍を加算
$$
{x_1}+2{x_2}=3  　←-{x_2}=-4を加算
$$
$$
→{x_1}=-1
$$
    上記手順を行基本変形といい、行列にも同じ概念が適用できる。
    行基本変形は行列を変形する行為とも言える。
    行列の場合、行基本変形は行列を左からかける。
    最終的に単位行列の状態になるまで変形を続けることにより、
    未知のベクトルx1,x2を求めることができる。
$$
\begin{pmatrix}
1 & 2 \\
2 & 5
\end{pmatrix}
\begin{pmatrix}
{x_1} \\ 
{x_2}
\end{pmatrix}
=
\begin{pmatrix}
3 \\
8
\end{pmatrix}
$$
    1.2行目に1行目の-2倍を加算（する行列を左からかける）
$$
\begin{pmatrix}
1 & 0 \\
-2 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 2 \\
2 & 5
\end{pmatrix}
\begin{pmatrix}
{x_1} \\ 
{x_2}
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
-2 & 1
\end{pmatrix}
\begin{pmatrix}
3 \\
8
\end{pmatrix}
$$
$$
→
\begin{pmatrix}
1 & 2 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
{x_1} \\ 
{x_2}
\end{pmatrix}
=
\begin{pmatrix}
3 \\
2
\end{pmatrix}
$$
    2.1行目に2行目の-2倍を加算（する行列を左からかける）
$$
\begin{pmatrix}
1 & -2 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 2 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
{x_1} \\ 
{x_2}
\end{pmatrix}
=
\begin{pmatrix}
1 & -2 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
3 \\
2
\end{pmatrix}
$$
$$
→
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
{x_1} \\ 
{x_2}
\end{pmatrix}
=
\begin{pmatrix}
-1 \\
2
\end{pmatrix}
$$
    後述の逆行列により求めると...
$$
\begin{pmatrix}
5 & -2 \\
-2 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 2 \\
2 & 5
\end{pmatrix}
\begin{pmatrix}
{x_1} \\ 
{x_2}
\end{pmatrix}
=
\begin{pmatrix}
5 & -2 \\
-2 & 1
\end{pmatrix}
\begin{pmatrix}
3 \\
8
\end{pmatrix}
$$
$$
→
\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}
\begin{pmatrix}
{x_1} \\ 
{x_2}
\end{pmatrix}
=
\begin{pmatrix}
-1 \\
2
\end{pmatrix}
$$

### 単位行列と逆行列
    単位行列と逆行列について
$$ 
\begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix}
\begin{pmatrix}
{x_1} \\ 
{x_2}
\end{pmatrix}
=
\begin{pmatrix}
-3 & 2 \\
1 & -\frac{1}{2} 
\end{pmatrix}
\begin{pmatrix}
7 \\
10
\end{pmatrix}
$$
    単位行列：1が斜めに並んだ行列、掛けても元の数値を変異させない。
    逆行列：行列の割り算のようなもの。行列の逆数
    ただし表記する際は指数-1を行列の右上に付与するが、スカラーの逆数とは異なり分数表現には変換できない。
    また、基となる行列と逆行列を乗算すると単位行列が生成される。
$$　
AA^{-1}＝A^{-1}A=
\begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix}→I
$$
    連立方程式を行列として扱うことにより解くことができる。
    逆行列は行基本変形と行列を対応させれば求まる
    ただし、左辺は行列×ベクトル、右辺はベクトルのため
    →左右の形式を一緒にするために単位行列を右辺にかける
$$
\begin{pmatrix}
1 & 4 \\
2 & 6
\end{pmatrix}
\begin{pmatrix}
{x_1} \\
{x_2} 
\end{pmatrix}
=
\begin{pmatrix}
7 \\
10
\end{pmatrix}
$$
$$
→
\begin{pmatrix}
1 & 4 \\
2 & 6
\end{pmatrix}
\begin{pmatrix}
{x_1} \\
{x_2} 
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
7 \\
10
\end{pmatrix}
$$
    行基本変形を行うたびに元の行列だけでなく、
    右側の単位行列についても逐次更新を行う。
    あたかも行基本変形の全ての操作が蓄積・記録されたかのように
    右側に表現されている。これが逆行列に相当する。

$$　
\begin{pmatrix}
1 & 4 | 1 & 0 \\
2 & 6 | 0 & 1
\end{pmatrix}・・・元の行列（および右辺にかけた単位行列）
$$
$$
\begin{pmatrix}
1 & 4 | 1 & 0\\
1 & 3 | 0 & \frac{1}{2}
\end{pmatrix}  ・・・2行目を\frac{1}{2}倍
$$
$$
\begin{pmatrix}
0 & 1 | 1 & -\frac{1}{2} \\
1 & 3 | 0 & \frac{1}{2}
\end{pmatrix}・・・1行目に2行目の-1倍を加算
$$
$$
\begin{pmatrix}
0 & 1 | 1 & -\frac{1}{2} \\
1 & 0 | -3 & 2
\end{pmatrix}・・・2行目に1行目の-3倍を加算
$$
$$
\begin{pmatrix}
1 & 0 | -3 & 2 \\
0 & 1 | 1 & -\frac{1}{2}
\end{pmatrix}・・・1行目と2行目を入れ替える
\\→右側に逆行列が現れる。左辺は単位行列に変化
$$
    これらは掃き出し法と呼ばれる逆行列を求める手法の一つ
    ただし必ず逆行列が存在するわけではない。
    逆行列が存在しないケースというものがある。
    具体的には解がない、解が１つに定まらない連立方程式に相当するもの。
$$
例えば正方行列
\begin{pmatrix}
1 & 4 \\
1 & 4
\end{pmatrix}、
\begin{pmatrix}
1 & 4 \\
2 & 8
\end{pmatrix}、
\begin{pmatrix}
1 & 4 \\
0 & 0
\end{pmatrix}など
\\・・・
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}と定義した際、a:b=c:dが成り立つ場合\\
つまりad-bc=0の場合、逆行列は存在しない。\\
(ベクトルを平行四辺形の辺に見立てた場合、面積が0となる)
$$

### 行列式の特徴
    行列式（determinant）は正方行列の大きさ・量を表す
    ２つのベクトルで構成される平行四辺形の面積が逆行列の有無を判別する。
$$
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix} \\
行列式：ad-bc \\
$$
    1.同じベクトルが含まれていると0になる
        連立方程式で同じ成分が含まれていると解がない
    2.１つのベクトルがλ倍　→　全体に対しベクトルがλ倍
    3.１つのベクトルに加算　→　行列式の足し合わせになる
    4.ベクトルを入れ替えると符号が入れ替わる
    ３つ以上のベクトルからできている行列は展開できる
$$
3つ以上のベクトルから出来ている行列
\begin{vmatrix}
a & b & c \\
d & e & f \\
g & h & i \\
\end{vmatrix} \\
行列式：a
\begin{vmatrix}
e & f \\
h & i \\
\end{vmatrix} -
d
\begin{vmatrix}
b & c \\
h & i \\
\end{vmatrix} +
g 
\begin{vmatrix}
b & c \\
e & f \\
\end{vmatrix} \\
$$

### 固有値と固有ベクトル
    以下に示すように行列Aに特殊なベクトルxをかけたものと、スカラーλに特殊なベクトルxをかけたものが等しいとする。
    この等式が成り立つ際、ベクトルxとスカラーλは行列Aに対する固有ベクトル、固有値にあたる。
$$
A\vec{x}=λ\vec{x} \\
A:行列 \\
\vec{x}:固有ベクトル \\
λ:固有値 \\
\begin{pmatrix}
1 & 4 \\
2 & 3
\end{pmatrix}
\begin{pmatrix}
1 \\
1
\end{pmatrix}
=
5
\begin{pmatrix}
1 \\
1
\end{pmatrix} \\

固有ベクトル(の一つ)=
\begin{pmatrix}
1  \\
1 
\end{pmatrix} （の定数倍）\\
固有値=5
$$
    上記等式から行列Aとスカラーλは等価であることが分かる。
    なお、固有値は明確な値に定まるが 固有ベクトルは１つに定まらない。
    固有ベクトルはある特定の比率を表すベクトルであり、「～の定数倍」といった表現をする。
$$
A\vec{x}=λ\vec{x} \\
(A-λI)\vec{x}=\vec{0} \\
\vec{x}≠\vec{0} \\
固有ベクトルは\vec{0}ではない
$$
    固有値・固有ベクトルの求め方
    1.行列の対角成分からλを差し引いた行列を定義
$$
|A-λI|=0 \\
\begin{vmatrix}
1-λ & 4 \\
2 & 3-λ
\end{vmatrix}=0 \\
$$
    2.行列式を求める
$$
(1-λ)(3-λ)-4・2=0 \\
3-4λ+λ^2-8=0 \\
3-4λ+λ^2-8=0 \\
λ^2-4λ-5=0 \\
二次方程式の公式に当てはめて・・・\\
λ=\frac{4\plusmn\sqrt{16+20}}{2} \\
=\frac{4\plusmn6}{2} \\
=5,-1 \\
→固有値が求まった
$$
    3.固有値λを元の式に代入して、ベクトルxを求める
        固有値λ=5の場合
$$
\begin{pmatrix}
1 & 4 \\
2 & 3
\end{pmatrix}
\begin{pmatrix}
{x_1} \\
{x_2}
\end{pmatrix}
=
5
\begin{pmatrix}
{x_1} \\
{x_2}
\end{pmatrix} \\
→1{x_1}×4{x_2}+2{x_1}×3{x_2}=5{x_1}+5{x_2} \\
→3{x_1}+7{x_2}=5{x_1}+5{x_2} \\
→-2{x_1}=-2{x_2} \\
→{x_1}={x_2} \\
よって{x_1}=1,{x_2}=1 \\
\begin{pmatrix}
{x_1} \\
{x_2}
\end{pmatrix} 
=
\begin{pmatrix}
1 \\
1
\end{pmatrix} (の定数倍)
\\
$$
        固有値λ=-1の場合も同様に、
$$
\begin{pmatrix}
1 & 4 \\
2 & 3
\end{pmatrix}
\begin{pmatrix}
{x_1} \\
{x_2}
\end{pmatrix}
=
-1
\begin{pmatrix}
{x_1} \\
{x_2}
\end{pmatrix} \\
→1{x_1}×4{x_2}+2{x_1}×3{x_2}=-1{x_1}-1{x_2} \\
→3{x_1}+7{x_2}=-1{x_1}-1{x_2} \\
→4{x_1}=-8{x_2} \\
→{x_1}=-2{x_2} \\
よって{x_1}=2,{x_2}=-1 \\
\begin{pmatrix}
{x_1} \\
{x_2}
\end{pmatrix} 
=
\begin{pmatrix}
2 \\
-1
\end{pmatrix}  (の定数倍)
\\
$$

### 固有値分解
    すべて実数で構成される正方行列Aが固有値 λ1,λ2･･･と固有ベクトルv1,v2･･･を持っていると仮定する。
    この固有値を対角線上に並べた行列Λ（対角線上以外の成分は0）
$$
\Lambda=
\begin{pmatrix}
{\lambda_1} & 0 & 0 & ... & 0\\
0 & {\lambda_2} & 0 & ... & 0\\
... & ... & ... & ... & ...\\
0 & 0 & 0 & ... & {\lambda_n}\\
\end{pmatrix} 
$$
    と，それに対応する固有ベクトルを並べた行列𝑉
$$
V=
\begin{pmatrix}
{v_1} & 0 & 0 & ... & 0\\
0 & {v_2} & 0 & ... & 0\\
... & ... & ... & ... & ...\\
0 & 0 & 0 & ... & {v_n}\\
\end{pmatrix} 
$$
    を定義したとき，関係性は以下の式で表される。
$$
𝐴V=V\Lambda
$$
    この式を変形すると、行列Aは(固有ベクトル行列)×(固有値行列)×(固有ベクトル行列の逆行列)となる。
$$
𝐴=V\Lambda V^{-1}
$$
    このように正方行列を３つの行列の積に変換することを固有値分解という。
    この変換により行列の累乗の計算が容易になる等の利点がある。
    ただし、あくまで正方形の行列にのみ適用できる手法であり、
    長方形の行列などは後述の特異値分解により、類似したことが実現できる。
    固有値分解の手順としては、以下の通り
    1.固有値を求める
    2.固有ベクトルを求める
    3.固有ベクトルの逆行列を求める

### 特異値分解
    正方行列以外の場合、固有値分解はできないが、類似したことは実現可能。
    以下のような特殊な単位ベクトルがあれば特異値分解できる。
$$
M\vec{v} = \sigma\vec{u} \\
M^{T}\vec{u} = \sigma\vec{v} \\
行列を転置し、左右のベクトルを交換しても等式が成り立つ \\
M=USV^{-1} \\
M:行列(正方行列ではない) \\
S:特異値(\sigmaの頭文字) \\
\vec{U}:左特異ベクトル \\
\vec{V}:右特異ベクトル
$$
    特異値の求め方
    長方形の行列とその転置行列をかけ合わせた場合、正方行列が生まれる。
    この正方行列に対して固有値分解することにより、特異値分解が実現できる。
$$
MV=US \qquad M^{T}U=VS^{T} \\
M=USV^{-1} \qquad M^{T}=VS^{T}U^{-1} \\
$$
    行列Mとその転置行列M^Tの積は以下の通り、固有値分解と類似した３つのベクトルの積に相当する。
    ２つ目のベクトルは特異値Sの２乗であり、この平方根を取れば特異値Sが導出できる。
    Uは左特異ベクトルに相当する。    
$$
MM^{T}=USV^{-1}VS^{T}U^{-1} \\
=USS^{T}U^{-1} \\
=U・SS^{T}・U^{-1}
$$
    特異値分解の手順
    1.行列とその転置行列を掛け合わせる
    2.固有値分解を行う
    3.固有値の平方根を取る
$$
M=\begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix} \\
MM^{T} = \begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 3 \\
2 & 2 \\
3 & 1 \\
\end{pmatrix} \\
=\begin{pmatrix}
1+4+9 & 3+4+3 \\
3+4+3 & 9+4+1 \\
\end{pmatrix} \\
=\begin{pmatrix}
14 & 10 \\
10 & 14 \\
\end{pmatrix} ・・・正方行列\\
\begin{pmatrix}
14 & 10 \\
10 & 14 \\
\end{pmatrix}
\begin{pmatrix}
{x_1} \\
{x_2} \\
\end{pmatrix}
=
\lambda
\begin{pmatrix}
{x_1} \\
{x_2} \\
\end{pmatrix}\\
\begin{pmatrix}
14-\lambda & 10 \\
10 & 14-\lambda \\
\end{pmatrix}
\begin{pmatrix}
{x_1} \\
{x_2} \\
\end{pmatrix}
=0\\
\lambda^{2}-28\lambda+196-100=0 \\
\lambda^{2}-28\lambda+96=0 \\
\frac{-b\plusmn\sqrt{b^{2}-4ac}}{2a}\\
=\frac{28\plusmn\sqrt{784-384}}{2}\\
=\frac{28\plusmn20}{2}=24,4 \\
 ・・・固有値(特異値の二乗) \\
\begin{pmatrix}
14 & 10 \\
10 & 14 \\
\end{pmatrix}
\begin{pmatrix}
{x_1} \\
{x_2} \\
\end{pmatrix}
=
4
\begin{pmatrix}
{x_1} \\
{x_2} \\
\end{pmatrix}
\\
14{x_1}+10{x_2}+10{x_1}+14{x_2}=4{x_1}+4{x_2} \\
20{x_1}=-20{x_2} \\
{x_1}=-{x_2} \\
よって{x_1}=-1,{x_2}=1 \\
\begin{pmatrix}
14 & 10 \\
10 & 14 \\
\end{pmatrix}
\begin{pmatrix}
{x_1} \\
{x_2} \\
\end{pmatrix}
=
24
\begin{pmatrix}
{x_1} \\
{x_2} \\
\end{pmatrix}\\
14{x_1}+10{x_2}+10{x_1}+14{x_2}=24{x_1}+24{x_2} \\
(24-24){x_1}=(24-24){x_2} \\
{x_1}={x_2}\neq0 \\
よって{x_1}=1,{x_2}=1 \\
\begin{pmatrix}
1 & -1 \\
1 & 1 \\
\end{pmatrix}・・・左特異ベクトル\\
$$
    特異値分解の用途について
    ・学習用のデータのサイズ削減
        (成分の小さいデータを取り除き、データ量を削減)
### 要点まとめ(第1章)
    行列についての学習全体を通じ、本格的に機械学習に取り組んでいく上で重要と感じた点を３点挙げる。
    まず1つ目は行列を用いた計算は、ある入力値に対して何らかのパラメータを与えて、新たな成分を生み出す、言わば変換のための手法であると理解した。
    転置や逆行列といった概念もその変換を行うためのパーツにすぎないもの、と捉えている。
    2つ目は、その新たな成分は入力値の複数の成分を基に導き出されるということ。これはニューラルネットワークを図示したものを思い浮かべると合点がいった。
    3つ目は固有値分解、特異値分解といった何らかの結果を得るものであり、HowToをなぞるだけでなく得られた結果に基づきどのような活用・利点があるかを十分理解する必要があると感じた。
    

## 第2章：確率・統計
### 集合について
    集合とはものの集まり
    a,b,c,d,e,f,gという要素の集まりを集合Sとして定義すると・・・
    数学的には、以下のような表記となる
$$
S=｛a,b,c,d,e,f,g｝ \\
$$
    集合の要素（元）同士は明確に区別できる
$$
a\isin S ・・・aはSに属する\\
b\isin S ・・・bはSに属する\\
$$
    集合Sの内部にM={c,d,g}がある場合、
$$
M\subset S ・・・MはSに属する\\
$$
    集合に含まれる・含まれないは明確に区別できる
$$
h\notin S ・・・hはSに属さない\\
$$
    確率・統計で扱う「事象」は「集合」として取り扱うことができる。
### 和集合と共通部分
    和集合
$$
A \cup B・・・AもしくはB（A cup B）
$$
    共通部分
$$
A \cap B・・・AかつB（A cap B）
$$
### 絶対補と相対補
$$
U\backslash A =\bar A・・・全てからAを除いたもの \\
→絶対補
$$
$$
B\backslash A・・・BからAを除いたもの \\
→相対補
$$

### 
$$
1.\overline{A\cup B} \\
$$
    AにもBにも属さない
$$
2.\overline{A\cap B} \\
$$
    AとBの共通部分以外
$$
3.(B\backslash A) \cap (A\backslash B) \\
$$
    BからAを除いたものとAからBを除いたものの共通部分
    →そんなものはありえない
$$
4.(B\backslash A) \cup (A\backslash B) \\
$$
    BからAを除いたものとAからBを除いたものの和集合
    AとBの和集合から共通部分を除いたものに等しい

### 頻度確率とベイズ確率

    頻度確率(客観確率)
    •発生する頻度、確からしさ
    •例:「10本のうち一本だけ当たりのクジを引いて当選する確率を調べたところ10%であった」という事実

    ベイズ確率(主観確率)
    •信念の度合い、算出方法が難しい
    •例:「あなたは40%の確率でインフレンザです」という診断・信念
    →いろいろな条件を決めて確率を求める

### 確率の定義
$$
P(A) = \frac{n(A)}{n(U)} = \frac{事象Aが起こる場合の数}{すべての事象の数}
$$
    P：確率（Properbility）
    A：事象（イベント）
    すべての事象が起きた場合は分母と分子が同じになるので1となる。
    確率は0～1をとる。
$$
P(\bar A)を使ってP(A)を表現すると　\\
P(\bar A)=1-P(\bar A) \\
=\frac{事象Aが起こらない数}{全ての事象の数} \\
=\frac{全ての事象の数-事象Aの起こる数}{全ての事象の数} \\
=\frac{n(U)-n(A)}{n(U)} \\
=\frac{n(U)}{n(U)}-\frac{n(A)}{n(U)} \\ 
=1-P(A)
$$
$$
P(A\cap B)→AとBの共通部分の確率 \\
=P(A)P(B|A) 
P(A)=P(A|U) →すべての事象のもとAの発生する確率 \\
P(B|A)      →Aという条件のもとBの発生する確率 \\
\qquad \\ 
P(A\cap B)とP(B\cap A)は同じこと \\
P(A\cap B)=P(B\cap A) \\
P(A)P(B|A) = P(B)P(A|B) \\
Aの発生する確率×Aという条件のもとBの発生する確率 = Bの発生する確率×Bという条件のもとAの発生する確率 \\
$$
### 条件付き確率
    同時確率と条件付確率の違い
    同時確率
        全ての事象において、ある事象Bと事象Aが同時に起きる確率
        例:雨が降るのと、交通事故が同時に発生する確率
    条件付き確率
        ある事象Bが与えられた下で，Aが起きる確率
        例:雨が降っている条件下で交通事故に遭う確率
$$
P(A|B)=\frac{P(A\cap B)}{P(B)}\\
=\frac{n(A\cap B)}{n(B)} \\
 \\
P(A\cap B)・・・全ての事象の中で、雨が降るという事象と交通事故に合う事象が同時に起こる確率（同時確率） \\
\frac{P(A\cap B)}{P(B)})・・・雨が降っている条件下で交通事故に遭う確率（条件付確率） \\
 →条件付確率の方が大きくなる
$$
### 独立な事象の同時確率
    お互いの発生には因果関係のない事象Aと事象Bが同時に発生する確率
$$
P(A\cap B) = P(A)P(B|A) \\
=P(A)P(B) \\
$$
    猫を見る(A)と風邪をひく(B)
    →因果関係がない(AのせいでBがおこる、BのせいでAがおこるわけではない)

### 確率の和集合
$$
P(A\cup B)・・・AまたはBの発生する確率 \\
=P(A)+P(B)-P(A\cap B)\\
=事象Aの発生する確率 \\
+事象Bの発生する確率 \\
-事象AとBが同時に発生する確率 \\
\qquad \\ 
→P(A\cap B)を二重に数えているので重複を除去している。
$$
### ベイズ則について
    ある街の子どもたちは毎日1/4の確率で飴玉をもらうことができ，飴玉をもらうと1/2の確率で笑顔になるという。
    (4日に1回もらえているという統計に基づく)

    その街の，笑顔な子どもが飴玉をもらっている確率を求めよ。(ただし，この街の子どもたちが笑顔でいる確率は1/3である。)

    事象A(飴玉をもらえる)
    事象B(笑顔になる)
$$
P(A)P(B|A) = P(B)P(A|B) \\
$$
    飴玉をもらえる条件で笑顔になる条件
    ＝
    笑顔になる条件で飴玉をもらえる条件

$$
P(飴玉)P(笑顔|飴玉) = P(笑顔)P(飴玉|笑顔) \\
P(飴玉)=\frac{1}{4} \\
P(笑顔|飴玉)=\frac{1}{2} \\
P(笑顔)=\frac{1}{3} \\
\qquad \\ 
P(笑顔|飴玉)×P(飴玉)=P(飴玉,笑顔)=P(飴玉 \cap 笑顔) \\
\frac{1}{4}×\frac{1}{2}=\frac{1}{8} \\
→笑顔になる＆飴玉をもらえるの同時確率 \\
\qquad \\ 
P(飴玉,笑顔)=P(笑顔,飴玉)=P(笑顔 \cap 飴玉) \\
\frac{1}{8}=\frac{1}{8}=\frac{1}{8} \\
→同時確率はAとBが逆転しても同じ \\
\qquad \\ 
P(飴玉,笑顔)=P(飴玉|笑顔)×P(笑顔) \\
\frac{1}{8}=P(飴玉|笑顔)×\frac{1}{3} \\
→P(飴玉|笑顔)=\frac{1}{8}×\frac{3}{1} = \frac{3}{8}\\
→笑顔の子供のうち、飴玉をもらった子供の確率は\frac{3}{8}である。
$$
### 統計
    統計には２つの大きなカテゴリがある
    ・記述統計
        集団の性質を要約して記述する。
        例)エレベータのゴンドラの設計
        母集団から平均的な人間の体重を把握して上限を決めていく
        木ではなく森を見る必要がある場合
    ・推測統計
        全体の集合である母集団から一部のサンプル（標本）を抽出
        標本から逆算して元の母集団を推測する。
        例）工場で生成された製品の不具合チェック、品質管理
    本講では主に記述統計に関する内容について述べられている。
    （ビッグデータの活用など、まさに記述統計の概念に基づく）

### 確率変数と確率分布
    確率変数
        ・事象と結び付けられた数値
        ・事象そのものを指すと解釈する場合も多い
    確率分布
    ・事象の発生する確率の分布
    ・離散値であれば表に示せる
|事象|裏が0枚<BR>表が4枚|裏が1枚<BR>表が3枚|裏が2枚<BR>表が2枚|裏が3枚<BR>表が1枚|裏が4枚<BR>表が0枚|
|----|----|----|----|----|----|
|確率変数|4|3|2|1|0|
|事象が発生した回数<BR>総計1,200回中|75|300|450|300|75|
|事象と対応する確率|1/16|4/16|6/16|4/16|1/16|

### 期待値
    期待値とは
    その分布における，確率変数の平均値　もしくは 「ありえそう」な値
    　→確率的に高い分布＝平均値

|事象X|x1|x2|...|xn|
|----|----|----|----|----|
|確率変数f(X)|f(x1)|f(x2)|...|f(xn)|
|確率P(X )|P(x1)|P(x2)|...|P(xn)|
    確率変数×確率によって期待値（≒平均）が求まる。
    くじ引きの例で、
    賞金の額（確率変数）が50万
    賞金の当たる（確率）が1/2
    50万×1/2=25万・・・これが期待値
$$
期待値E(f)={\displaystyle \Sigma_{k=1}^{n}}P(X={x_k})f(X=
{x_k}) \\
\qquad \\
連続する値なら...期待値E(f)=\int P(X={x_k})f(X={x_k})dx
$$
### 分散と共分散
    分散とは
    ・データの散らばり具合
    ・データの各々の値が，期待値からどれだけズレているのか平均したもの
    ・確率変数と期待値の差分を二乗したもの
    ※二乗する理由・・・元々絶対値をつけていたが計算コストが高く、二乗によって容易に符号が取りのぞけるため
    例)ある会社（10個の支店）の売り上げが平均したら100万円だった。。。
    全ての支店が100万円ではない、支店によって偏りがあるはず。
    支店ごとのばらつきがあれば分散も大きくなる。
$$
分散Var(f)=E(({f(X=x)}-E(f))^2) \\
=E({f_(X=x)^2})-(E(f))^2 \\
=確率変数の二乗の平均-期待値の二乗
$$
    共分散とは
    ・２つのデータ系列の傾向の違い
    ・正の値を取れば似た傾向
    ・負の値を取れば逆の傾向
    ・ゼロを取れば関係性に乏しい(年収と名前の画数の関係)
$$
共分散Cov(f,g)=E(({f_(X=x)}-E(f))({g_(Y=y)}-E(g))) \\
=E(fg)-E(f)E(g)
$$
### 分散と標準偏差
    分散は２乗してしまっているので元のデータと単位が違う
    分散のままでは不都合がある。。。
    ※身長の計測：二乗を取っているので単位が変わってしまう！！
    2乗することの逆演算（つまり平方根を求める）をすれば元の単位に戻る
    小文字のシグマで表す
    標準偏差が小さい＝分散も小さい＝ばらつきが少ない
$$
\sigma=\sqrt{Var(f)}=\sqrt{E(({f_(X=x)}-{E_(f)})^2)}
$$

### ベルヌーイ分布とカテゴリカル分布
    ベルヌーイ分布・・・重要
    ・コイントスのイメージ
    ・裏と表で出る割合が等しくなくとも扱える
$$
P(x|\mu)=\mu^x(1-\mu)^{1-x} \\
\mu・・・平均の値(確率)、xが1(コインの表)の場合の確率 \\
\mu=\frac{2}{3} \\
x=1の場合 \\
\mu^x(1-\mu)^{1-x} = \frac{2}{3}^1(1-\frac{2}{3})^{1-1} = \frac{2}{3}\\
x=0 \\
\mu^x(1-\mu)^{1-x} = \frac{2}{3}^0(1-\frac{2}{3})^{1-0} = \frac{1}{3}\\
$$
    マルチヌーイ（カテゴリカル）分布
    ・さいころを転がすイメージ
    ・各面の出る割合が等しくなくとも扱える
    式は基本的にはベルヌーイ分布の考え方に基づく。
    出る目の数が違うだけ
### 二項分布とガウス分布
    二項分布
    ・ベルヌーイ分布の多試行版
    二項係数を用いている。
    全体n個のうちからx個だけ取り出す。
    コイントスn回中、表がx回出た、といった場合。
    （コイントス5回中、2回表が出た（1回目と5回目））
    真ん中あたりが一番大きくなる。(山のような形の分布)
$$
P(x|\lambda,n)=\frac{n!}{x!(n-x)!}\lambda^x(1-\lambda)^{n-x}
$$
    ガウス分布
    ・釣鐘型の連続分布
    真の分布が分からなくてもサンプルが多ければ正規分布に近づく
    分散が大きければ山が広がる。
    絶対値をとるために二乗している。
$$
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2) \\
\sigma^2・・・分散を表している  \\
面積を全てを足し合わせるとちょうど１になる
$$
### 推定
    母集団を特徴づける母数（パラメータ：平均や分散）を統計学的に推測すること。
    注意：母集団≠母数
    １．点推定（今回取り上げるのはこちら）
        平均値などを1つの値に推定すること
    ２．区間推定
        平均値などが存在する範囲（区間）を推定すること
    
    推定量と推定値
        推定量（estimator)：パラメータを推定するために利用する数値の計算方法た計算式のこと。推定関数ともいう。
        （中に関数が入っている）

        推定値（estimate)：実際に試行を行った結果から計算した値。推定量の計算式を用いて求めた結果
$$
真の値を\thetaとすると・・・\hat{\theta}のように表す \\
\hat{\theta}(x)→推定量 \\
\hat{\theta}→推定値 \\
$$
### 標本平均
    標品平均・・・点推定の代表的なもの（おおまかな特徴を捉えやすい）
    母集団から取り出した標本の平均値
    サンプル数が大きくなれば母集団の値に近づく→一致性
    サンプル数がいくらであってもその期待値は母集団の値と同様→不偏性（偏りがない）
$$
E(\hat{\theta})=\theta
$$

### 標本分散
    サンプルサイズをnとすると、
$$
\hat{\sigma}^2=\frac{1}{n}{\displaystyle \Sigma_{i=1}^{n}}({x_i}-\bar{x})^2 \\
※\bar{x}は平均値(E(\hat{\theta}))
$$
    一致性は満たすが、不偏性は満たされない。(不偏性が小さくなる)
    ・たくさんのデータのばらつき具合
    ・少数のデータのばらつき具合
        →どちらがばらつくか？
        一部のサンプルでは元々の母集団のもつ値のばらつきよりも大きくはならない。
    標本分散を母集団の持つ真の分散に近づけるためにはどうしたらよいか？
    →不偏分散
$$
s^2=\frac{n}{n-1}×\frac{1}{n}{\displaystyle \Sigma_{i=1}^{n}}({x_i}-\bar{x})^2 \\
なぜ\frac{n}{n-1}という数値をかけるのか？\\
小さくなった不偏性を大きく補正する \\
$$
### 要点まとめ(第2章)
    集合と確率、統計の学習全般に関して要点をまとめる。
    集合の概念は、日常の中で物事の大きさや範囲を言葉で表現して意思疎通している事柄をより数学的に明確に表し、解釈によって理解が異なることを防ぐことができる。普段何気なくベン図を用いていたが、より理解が深まった。
    確率において、同時確率と条件付き確率の違いについて当初なかなか理解が進まなかったが、普段課題として捉えることが多いのは条件付き確率であり、同時確率の概念を通じて事象AとBの位置づけを変換できる、という点が目から鱗だった。過去の経験からつい物事を一般化しがちだが、確率の概念を正しく持ち込めば思い込みを払拭することができると感じた。
    統計に関して、集めたデータの傾向を把握するもの、というざっくりとした理解はあったが、推定に関してはあまり理解がなかった。与えられた命題に対して求める期待値が、分布図上の平均値と同義である、と解釈した。統計学上の推定の概念がこれから学ぶ機械学習・ディープラーニングの原点にあたるものと捉えている。

## 第3章：情報理論
### 情報科学
    情報をどうやって数量化するのか？
    枠内の点の数、どちらが多いか
    X:11個とW:10個・・・すぐには分からない
    X:2個とW:1個・・・瞬時にわかる
$$
どちらも変化量\Delta W=1 \\
→元の母集団の数に対する変化量の割合で表現してみては\\
\frac{\Delta W}{W}=\frac{1}{10} \\
\quad \\
\frac{\Delta W}{W}=\frac{1}{1} \\
→増加の比率が違う
$$
    手に乗せた100gのおもりを110gにすると重くなったことに気づく
    何もない手の平に10gのおもりを載せても気づかない
    Wは少々使いづらい・・・確率Pで表せないのか？
### 自己情報量
$$
自己情報量：I(x)=-log(P(x))=log(W(x))
$$
    ・対数の底が2のとき，単位はビット(bit)
    ・対数の底がネイピアのeのとき，単位は(nat)
    Wは自然科学的だが、自己情報量は工学的    
    2個分の複雑さを表現・・・ONとOFF→スイッチ1個分で良い
    4個分の複雑さを表現・・・ONとOFF→スイッチ2個分で良い
    8個分の複雑さを表現・・・ONとOFF→スイッチ3個分で良い
        →logをとれば導き出せる
    起きる確率の高い事象・・・情報量少ない
    起きる確率の低い事象・・・情報量が多い
### シャノンエントロピー
    ・微分エントロピーともいうが，微分しているわけではない
    ・自己情報量の期待値、マイナス値をとることはない
    ・ある情報源がどの程度の情報を出しているか、を表す尺度ともいえる
$$
H(x)=E(I(x))\\
=-E(log(P(x)))\\
=-\Sigma(P(x)log(P(x))) \\
I(x):事象Xの自己情報量
$$
    情報量が最大になる場合を考える。それは珍しい、起こりにくいケース。
    機械学習の誤差関数の中身に使える。

### カルバック・ライブラーダイバージェンス
    ・同じ事象・確率変数における異なる確率分布P,Qの違いを表す
    元々考えられていた分布Qに対して指向の結果Pという分布が生まれた場合
    その差を把握するのに用いる
    2つの分布の違いを距離に見立てた数値で表したもの
    ただし、P→QとQ→Pの距離（差異）は異なる。
    また、マイナス値をとることはない。
    距離がゼロになるように学習させたい（誤差関数に用いられる）
$$
{D_{KL}}(P||Q)=E_{x～P} [log\frac{P(x)}{Q(x)}] = E_{x～P}[logP(x)-logQ(x)] \\
$$
    自己情報量I(Q(x)と自己情報量I(P(x))の差を求める
$$
I(Q(x))-I(P(x))=(-log(Q(x)))-(-log(P(x)))=log\frac{P(x)}{Q(x)}
$$
    Q(x)・・・事象xが発生する確率Q(最初に把握していた珍しさ)
    P(x)・・・事象xが発生する確率P(後から分かった珍しさ)
$$
E(f(x))={\displaystyle \Sigma_{x}}P(x)f(x) \\
$$
    f(x)・・・事象xの確率変数
    P(x)・・・事象xが発生する確率P
    E(f(x))・・・期待値（平均値）
$$
{D_{KL}}(P||Q)={\displaystyle \Sigma_{x}}P(x)(-log(Q(x))-(-log(P(x))) \\
={\displaystyle \Sigma_{x}}P(x)log\frac{P(x)}{Q(x)} \\
→Q(X)で除算する箇所を除けばシャノンエントロピーを求めているのと同じ！！
$$

### 交差エントロピー
    ・カルバック・ライブラーダイバージェンスの一部分を取り出したもの
    ・Qについての自己情報量をPの分布で平均している
    モールス信号（19世紀）当時は少ない情報しか送れない
    暗号表を事前に送付し、事前に意味が分かるようにした（情報量の圧縮）
    →あくまで予想なので暗号表にない信号を送る必要が生じた。。。
    事前に送る暗号表の自己情報量の分布：Q
    現実にはPで平均を取る。

$$
{D_{KL}}(P||Q)={\displaystyle \Sigma_{x}}P(x)(-log(Q(x))-(-log(P(x))) \\
$$
    H・・・シャノンエントロピー（自己情報量の期待値＝平均）
$$
H(P,Q)=H(P)+{D_{KL}}({P||Q})) \\
H(P,Q)=-{E_{x～P}}logQ(x)={\displaystyle \Sigma_{x}}P(x)logQ(x) \\
※KLダイバージェンスが限りなく小さくなれば、H(P,Q)とH(P)は等しいと言える。
$$

### 要点まとめ(第3章)
    情報科学のカテゴリでは自己情報量およびエントロピーという概念に触れた。
    自己情報量は確率が低ければ大きくなり、逆に高ければ小さくなるというもので、それは珍しい事象＝価値が高い情報といえる。
    シャノンエントロピーはある確率分布の自己情報量の期待値であるとすると、
    KLダイバージェンスは２つの確率分布のそれぞれの自己情報量の差の期待値となる。
    また、交差エントロピーは２つの確率分布がどの程度乖離しているか（近いしいか）を表す尺度で、これはある結果を推定する機械学習のアプローチ（誤差関数の最小化）で用いられる。
    誤差関数を用いて交差エントロピーを最小化していく、ということはKLダイバージェンスを最小化して限りなく対象の確率分布のシャノンエントロピーに近づく＝求めている正解が導き出せる、と理解した。

